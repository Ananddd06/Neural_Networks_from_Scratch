# ğŸ§  Neural Networks from Scratch

Welcome to **Neural Networks from Scratch**!  
This repository is your structured guide to **understanding, building, and training neural networks from the ground up** â€” without high-level libraries.

Weâ€™ll cover everything: ğŸ”¢ math â†’ ğŸ—ï¸ implementation â†’ ğŸ¤– applications.

---

## ğŸ“š Table of Contents

1. ğŸŒŸ Introduction to Neural Networks
2. ğŸ§¬ Biological Inspiration
3. ğŸ”— Neurons & Perceptrons
4. âš¡ Activation Functions
5. ğŸ”„ Forward Propagation
6. ğŸ“‰ Loss Functions
7. ğŸ”§ Gradient Descent & Backpropagation
8. ğŸš€ Optimization Algorithms
9. ğŸ›¡ï¸ Regularization Techniques
10. ğŸ—ï¸ Deep Neural Networks
11. ğŸ–¼ï¸ Convolutional Neural Networks (CNNs)
12. â³ Recurrent Neural Networks (RNNs)
13. ğŸ‹ï¸ Training Best Practices
14. ğŸ§© Projects & Applications
15. ğŸ“– References & Further Reading

---

## ğŸŒŸ 1. Introduction to Neural Networks

- ğŸ¤” What is a Neural Network?
- ğŸ•°ï¸ History (Perceptron â†’ Deep Learning)
- ğŸ› ï¸ Why build from scratch?

---

## ğŸ§¬ 2. Biological Inspiration

- ğŸ§  Neurons in the human brain
- âš–ï¸ Synaptic weights & signals
- ğŸ¤– Artificial neuron modeling

---

## ğŸ”— 3. Neurons & Perceptrons

- ğŸ§© Structure of a perceptron
- â• Weighted sum & bias
- â†”ï¸ Linear separability
- âŒ XOR problem limitations

---

## âš¡ 4. Activation Functions

- ğŸŸ¢ **Sigmoid** â†’ Probability mapping
- ğŸ”µ **Tanh** â†’ Centered outputs
- ğŸŸ  **ReLU** â†’ Sparse activations & fast learning
- ğŸŸ£ Leaky ReLU, ELU, GELU
- ğŸ§­ When to use which function

---

## ğŸ”„ 5. Forward Propagation

- ğŸ“Š Layer-wise computation
- ğŸ§® Matrix representation
- ğŸ“¦ Batch inputs
- ğŸ“ Example with a small NN

---

## ğŸ“‰ 6. Loss Functions

- ğŸ”¢ **Regression**: Mean Squared Error (MSE)
- ğŸ¯ **Classification**: Cross-Entropy Loss
- âš–ï¸ Hinge Loss, KL Divergence
- ğŸ¯ Intuition: why minimize loss?

---

## ğŸ”§ 7. Gradient Descent & Backpropagation

- ğŸŒ€ Gradient intuition
- ğŸ“ Derivatives of activation functions
- ğŸ”— Chain rule in backprop
- ğŸš§ Vanishing & exploding gradients

---

## ğŸš€ 8. Optimization Algorithms

- ğŸ“¦ Batch Gradient Descent
- ğŸ² Stochastic Gradient Descent (SGD)
- ğŸƒ Momentum
- ğŸ“‰ RMSProp
- âš¡ Adam Optimizer
- â³ Learning rate schedules

---

## ğŸ›¡ï¸ 9. Regularization Techniques

- ğŸ“‰ Overfitting vs Underfitting
- â– L1 / L2 Regularization
- ğŸ² Dropout
- ğŸ“Š Batch Normalization
- â±ï¸ Early Stopping
- ğŸ–¼ï¸ Data Augmentation

---

## ğŸ—ï¸ 10. Deep Neural Networks

- ğŸ—ï¸ Stacking multiple layers
- ğŸŒ Universal Approximation Theorem
- âš–ï¸ Initialization (Xavier, He)
- â†”ï¸ Depth vs Width trade-offs

---

## ğŸ–¼ï¸ 11. Convolutional Neural Networks (CNNs)

- ğŸ” Convolution operation
- ğŸ§© Filters & feature maps
- ğŸ“ Pooling layers
- ğŸ† Architectures: LeNet, AlexNet, ResNet

---

## â³ 12. Recurrent Neural Networks (RNNs)

- â±ï¸ Sequential data handling
- ğŸ”„ Vanilla RNNs
- ğŸ§  LSTM (Long Short-Term Memory)
- ğŸšª GRU (Gated Recurrent Units)
- ğŸ’¬ Applications: NLP, time-series

---

## ğŸ‹ï¸ 13. Training Best Practices

- ğŸ§¹ Data preprocessing & normalization
- ğŸ“¦ Mini-batch training
- ğŸ›ï¸ Hyperparameter tuning
- ğŸ“Š Metrics: Accuracy, F1, ROC-AUC
- ğŸ Debugging training issues

---

## ğŸ§© 14. Projects & Applications

- âœï¸ Handwritten digit recognition (MNIST)
- ğŸ–¼ï¸ Image classification
- ğŸ’¬ Sentiment analysis
- ğŸ“ˆ Time-series forecasting
- ğŸ¨ Neural style transfer

---

## ğŸ“– 15. References & Further Reading

- ğŸ“˜ **Books**
  - _Deep Learning_ â€“ Ian Goodfellow
  - _Neural Networks and Deep Learning_ â€“ Michael Nielsen
- ğŸ“ **Courses**
  - Andrew Ng â€“ Deep Learning Specialization
  - Stanford CS231n â€“ CNNs for Visual Recognition
  - MIT 6.S191 â€“ Intro to Deep Learning
- ğŸ“„ **Research Papers**
  - Perceptron (Rosenblatt, 1958)
  - Backpropagation (Rumelhart et al., 1986)
  - Deep Residual Learning (He et al., 2015)

---

## ğŸ¯ Goal

By completing this journey, you will:

- ğŸ§® Understand neural networks mathematically & conceptually
- ğŸ› ï¸ Implement networks using only **NumPy**
- ğŸ” Build intuition to debug & optimize deep learning models

âœ¨ Letâ€™s start building **Neural Networks from Scratch** ğŸš€
